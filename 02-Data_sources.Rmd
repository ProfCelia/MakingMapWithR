---
title: "Datenherkunft"
author: "Nico Hahn"
date: "25 8 2019"
output: html_document
---

# Datenherkunft
Ein Großteil der Daten, die in dieser Arbeit verwendet wurden, stammen aus OpenStreetMap. Je nach Größe wurden diese entweder mit der beigefügten OSM-App erstellt, oder von https://www.geofabrik.de/ heruntergeladen. Bei Zweiterem wurden die Datensätze mit den command line Tools **osmconvert** und **osmfilter** in ein sinnvolles Format konvertiert und gefiltert. Danach war nocheinmal eine weitere Transformation in das GEOJSON Format nötig, wofür das NodeJS Package **osmtogeojson** verwendet wurde. Zu diesen Datensätze zählen die Bäckereien in Europa und das Hamburger Straßennetzwerk.
Die bayerischen Verwaltungsgebiete wurden von https://opendata.bayern.de gedownloadet. Inkar ist ein Angebot der statistischen Ämter des Bundes und der Länder. Auf https://www.inkar.de/ können Datenbankabfragen zu verschiedenen Kennzahlen und Indikatoren erstellt werden. Diese wurden wie folgt mit den Shapefiles vereint:
```{r, eval=FALSE}
# Inkar Datensatz laden
eco <- read_csv("datasets/eco.csv")
# Shapefiles laden
shapes <- read_sf("datasets/lkr_ex.shp")
# Geocoding. Auskommentiert, da es wegen Request Limit ab und zu nicht funktioniert
# codes <- lapply(eco$Raumeinheit, geo_code)
# stattdessen environment laden
load("datasets/codes.RData")
# In Punkte umwandeln
points <- lapply(codes, function(x, ...) {
  if (length(x) > 0) {
    x
  } else {
    c(0, 0)
  }
})
eco$long <- unlist(lapply(points, function(x) x[1]))
eco$lat <- unlist(lapply(points, function(x) x[2]))
eco <- eco %>%
            st_as_sf(coords = c("long", "lat"), crs = 4326) %>%
            st_cast("POINT")
# Shapefiles crs transformieren
shapes <- st_transform(shapes, 4326)
# Fehlende Geocodings
shapes[-unlist(st_intersects(eco, shapes)), ]$BEZ_KRS
# manuelles Coding
eco[eco$Raumeinheit == "Amberg, Stadt", ]$geometry <-
  st_sfc(st_point(x = c(11.857951, 49.453090)), crs = 4326)
eco[eco$Raumeinheit == "Bayreuth, Stadt", ]$geometry <-
  st_sfc(st_point(x = c(11.576192, 49.940560)), crs = 4326)
eco[eco$Raumeinheit == "Hof, Stadt", ]$geometry <-
  st_sfc(st_point(x = c(11.904340, 50.309311)), crs = 4326)
eco[str_detect(eco$Raumeinheit, "Erlangen-H"), ]$geometry <-
  st_sfc(st_point(x = c(10.863692, 49.669239)), crs = 4326)
eco[str_detect(eco$Raumeinheit, "Augsburg, Stadt"), ]$geometry <-
  st_sfc(st_point(x = c(10.894387, 48.353622)), crs = 4326)
# Check
shapes[-unlist(st_intersects(eco, shapes)), ]$BEZ_KRS
# Spatial matching
eco <- eco[unlist(st_contains(shapes, eco)), ]
# check ob beide datensätze selbe reihenfolge haben
all(unlist(lapply(seq_len(nrow(eco)), function(x, ...) {
  st_contains(shapes[x, ], eco[x, ])
})) == 1)
# Eco erhält Polygon shapefiles
st_geometry(eco) <- shapes$geometry
eco <- eco[, c(2, 3, 12, 35, 38, 43, 51, 63, 80, 97, 110)]
write_sf(eco, "datasets/bavaria.shp")
```

Im Weiteren wurde dieser Datensatz immer als `bavaria.shp` geladen.

Folgende Datensätze wurden von Kaggle gedownloadet:
  
  * honey: Daten über die Honigproduktion in den USA von 1998-2012 (https://www.kaggle.com/jessicali9530/honey-production)
  * accidents: Autounfälle in UK von 2005-2017 (https://www.kaggle.com/tsiaras/uk-road-safety-accidents-and-vehicles)
  * abbreviations: Abkürzungen der Staaten in den USA (https://www.kaggle.com/giodev11/usstates-dataset)
  * us_energy_census: Energiedaten über die USA (https://www.kaggle.com/lislejoem/us_energy_census_gdp_10-14)

Das europäische Höhenraster stammt von der Europäischen Umweltbehörde: https://www.eea.europa.eu/data-and-maps/data/digital-elevation-model-of-europe

Die Flugdaten der USA wurden hier gedownloadet: http://stat-computing.org/dataexpo/2009/the-data.html
Diese enthalten Daten über alle Inlandsflüge in den USA von 1987-2008.

Die Daten für Denver wurden daraus wie folgt extrahiert:
```{r, eval = FALSE}
# NOT RUN
# Multiprocessing weil es sehr lang dauert
plan(multiprocess)
flights_usa <- future_map(
  paste("flight_data/", list.files("flight_data", ".csv"), sep = ""), read_csv
)
plan(sequential)
# Aus allen Datensätzen nur Flüge von Denver aus wählen
flights_denver <- lapply(flights_usa, function(x) {
  x[x$Origin == "DEN", ]
})
# Liste zu data.frame binden
flights_denver <- do.call(rbind, flights_denver)
# Nach Zielort zusammenfassen, Anzahl der Flüge zählen
flights_denver_g <- plyr::ddply(flights_denver, .(Dest), plyr::summarize,
                          no_of_flights = length(Dest))
# Flughäfen laden
airports <- read_csv("datasets/airports.csv")
# Spaltennamen für join ändern
colnames(flights_denver_g)[1] <- "iata"
# Flughafen und Flüge joinen
flights_denver_g <- plyr::join(flights_denver_g, airports, by = "iata")
# Nach Staat zusammenfassen, Anzahl der Flüge zu jeweiligem Ziel summieren
flights_denver_states <- plyr::ddply(flights_denver_g, .(state), plyr::summarize,
                               no_of_flights = sum(no_of_flights),
                               long = mean(long),
                               lat = mean(lat))
# Koordinaten für Denver hinzufügen
flights_denver_states$lat_denver <- airports[airports$iata == "DEN", ]$lat
flights_denver_states$long_denver <- airports[airports$iata == "DEN", ]$long
# speichern
write_csv(flights_denver_states, "datasets/denver.csv")
```

Von NaturalEarth wurde ein Datensatz mit den ShapeFiles der Welt heruntergeladen: https://www.naturalearthdata.com/downloads/50m-cultural-vectors/50m-admin-0-countries-2/

Von London Datastore ein Datensatz mit den Shapefiles von London: https://data.london.gov.uk/dataset/statistical-gis-boundary-files-london

Alle Datensätze, die nicht angesprochen wurden, sind entweder von OpenStreetMap gedownloadet oder wurden mit Hilfe der OpenStreetMap App erstellt.