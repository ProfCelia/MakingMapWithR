---
title: "Datenherkunft"
author: "Nico Hahn"
date: "25 8 2019"
output: html_document
---
```{r, echo=FALSE, include=FALSE}
library(stringr)
library(sf)
library(readr)
library(stplanr)
```

# Datenherkunft
Ein Großteil der Daten die in dieser Arbeit verwendet wurden stammen aus OpenStreetMap. Je nach Größe wurden diese entweder mit der beigefügten OSM-App erstellt oder von https://www.geofabrik.de/ heruntergeladen. Bei ZWeiterem wurden die Datensätze mit den Commandline Tools **osmconvert** und **osmfilter** in ein sinnvolles Format konvertiert und gefiltert. Danach war nochmal eine weitere Transformation in das GEOJSON Format nötig, wofür das NodeJS Package **osmtogeojson** verwendet wurde. Zu diesen Datensätze zählen die Bäkereien in Europa und das Hamburger Straßennetzwerk.
Die bayerischen Verwaltungsgebiete wurden von https://opendata.bayern.de gedownloadet. Inkar ist ein Angebot der statistischen Ämter des Bundes und der Länder. Auf https://www.inkar.de/ können Datenbankabfragen zu verschiedenen Kennzahlen und Indikatoren erstellt werden. Diese wurden wie folgt mit den Shapefiles vereint:
```{r, eval=FALSE}
# Inkar Datensatz laden
eco <- read_csv("datasets/eco.csv")
# Shapefiles laden
shapes <- read_sf("datasets/lkr_ex.shp")
# Geocoding. Auskommentiert, da es wegen Request Limit ab und zu nicht funktioniert
# codes <- lapply(eco$Raumeinheit, geo_code)
# stattdessen environment laden
load("datasets/codes.RData")
# In Punkte umwandeln
points <- lapply(codes, function(x, ...) {
  if (length(x) > 0) {
    x
  } else {
    c(0, 0)
  }
})
eco$long <- unlist(lapply(points, function(x) x[1]))
eco$lat <- unlist(lapply(points, function(x) x[2]))
eco <- eco %>%
            st_as_sf(coords = c("long", "lat"), crs = 4326) %>%
            st_cast("POINT")
# Shapefiles crs transformieren
shapes <- st_transform(shapes, 4326)
# Fehlende Geocodings
shapes[-unlist(st_intersects(eco, shapes)), ]$BEZ_KRS
# manuelles Coding
eco[eco$Raumeinheit == "Amberg, Stadt", ]$geometry <- st_sfc(st_point(x = c(11.857951, 49.453090)), crs = 4326)
eco[eco$Raumeinheit == "Bayreuth, Stadt", ]$geometry <- st_sfc(st_point(x = c(11.576192, 49.940560)), crs = 4326)
eco[eco$Raumeinheit == "Hof, Stadt", ]$geometry <- st_sfc(st_point(x = c(11.904340, 50.309311)), crs = 4326)
eco[str_detect(eco$Raumeinheit, "Erlangen-H"), ]$geometry <- st_sfc(st_point(x = c(10.863692, 49.669239)), crs = 4326)
eco[str_detect(eco$Raumeinheit, "Augsburg, Stadt"), ]$geometry <- st_sfc(st_point(x = c(10.894387, 48.353622)), crs = 4326)
# Check
shapes[-unlist(st_intersects(eco, shapes)), ]$BEZ_KRS
# Spatial matching
eco <- eco[unlist(st_contains(shapes, eco)), ]
# check ob beide datensätze selbe reihenfolge haben
all(unlist(lapply(seq_len(nrow(eco)), function(x, ...) {
  st_contains(shapes[x, ], eco[x, ])
})) == 1)
# Eco erhält Polygon shapefiles
st_geometry(eco) <- shapes$geometry
eco <- eco[, c(2, 3, 12, 35, 38, 43, 51, 63, 80, 97, 110)]
write_sf(eco, "datasets/bavaria.shp")
```

Im weiteren wurde dieser Datensatz immer als `bavaria.shp` geladen.
Die honey und Autounfall Datensätze wurde von Kaggle gedownloadet, https://www.kaggle.com/jessicali9530/honey-production und https://www.kaggle.com/tsiaras/uk-road-safety-accidents-and-vehicles. Außerdem wurde ein Datensatz mit den Abkürzungen der Staaten in den USA von Kaggle benutzt. (https://www.kaggle.com/giodev11/usstates-dataset)
Das europäische Höhenraster stammt von der Europäischen Umweltbehörde, https://www.eea.europa.eu/data-and-maps/data/digital-elevation-model-of-europe.
Die Flugdaten der USA wurden hier gedownloadet: http://stat-computing.org/dataexpo/2009/the-data.html.
Von NaturalEarth wurde ein Datensatz mit den ShapeFiles der Welt heruntergealden: https://www.naturalearthdata.com/downloads/50m-cultural-vectors/50m-admin-0-countries-2/.
Von London Datastore ein Datensatz mit den Shapefiles von London: https://data.london.gov.uk/dataset/statistical-gis-boundary-files-london.
Alle Datensätze die nicht angesprochen wurden sind entweder von OpenStreetMap gedownloadet oder wurde mit Hilfe der OpenStreetMap App erstellt.